<div align="center">
<pre>
     _   _   _             _   _               ___      
    / \ | |_| |_ ___ _ __ | |_(_) ___  _ __   |_ _|___  
   / _ \| __| __/ _ | '_ \| __| |/ _ \| '_ \   | |/ __| 
  / ___ | |_| ||  __| | | | |_| | (_) | | | |  | |\__ \ 
 /_/ _ \_\___\________| |_|\__|_|\___/__| |_| |___|____ 
    / \  | | | \ \ / ___  _   _  | \ | | ___  ___  __| |
   / _ \ | | |  \ V / _ \| | | | |  \| |/ _ \/ _ \/ _` |
  / ___ \| | |   | | (_) | |_| | | |\  |  __|  __| (_| |
 /_/   \_|_|_|   |_|\___/ \__,_| |_| \_|\___|\___|\__,_|
                                                        
</pre>
</div>
A PyTorch implentation of the Transformer architecture for the purpose of learning. 

# Transformer
The transformer was first introduced in the seminal paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). Although the paper's original intention was for language translation tasks, it has since been used in all machine learning tasks (from computer vision to natural language processing), revolutionizing the landscape of ml.

## Attention
The attention mechanism was first introduced 

## MutliHead Attention

## Encoder

## Decoder


# Training

## Dataset

# Testing